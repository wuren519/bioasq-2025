{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic==0.48.0\n",
    "%pip install beautifulsoup4==4.13.3\n",
    "%pip install dotenv==0.9.9\n",
    "%pip install huggingface-hub==0.29.3\n",
    "%pip install lxml==5.3.2\n",
    "%pip install matplotlib-inline==0.1.7\n",
    "%pip install ollama==0.4.7\n",
    "%pip install openai==1.65.1\n",
    "%pip install pandas==2.0.3\n",
    "%pip install regex==2024.11.6\n",
    "%pip install requests==2.32.3\n",
    "%pip install rich==13.9.4\n",
    "%pip install rouge==1.0.1\n",
    "%pip install safetensors==0.5.3\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install scipy==1.10.1\n",
    "%pip install scispacy==0.5.5\n",
    "%pip install sentencepiece==0.2.0\n",
    "%pip install spacy==3.7.5\n",
    "%pip install thinc==8.2.5\n",
    "%pip install tokenizers==0.20.3\n",
    "%pip install torch==2.4.1\n",
    "%pip install tqdm==4.67.1\n",
    "%pip install transformers==4.46.3\n",
    "%pip install typer==0.15.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from ollama import chatF\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "# import config\n",
    "def escape_for_json(input_string):\n",
    "    escaped_string = json.dumps(input_string)\n",
    "    return escaped_string\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#Suppress warnings about elasticsearch certificates\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "MINDATE = '2000/01/01'  # TODO: find a better starting date\n",
    "MAXDATE = '2025/01/01'  # TODO: BioASQ 12b requires us to use PubMed 2024 annual baseline version.\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# 请替换为你的实际API密钥\n",
    "API_KEY = \"your_key\"\n",
    "\n",
    "def run_elasticsearch_query(sentence, max_length=30, verbose=True):\n",
    "    \"\"\"Extract medical entities from a sentence and fetch relevant PubMed articles based on those entities.\"\"\"\n",
    "    \n",
    "    print(sentence)\n",
    "    \n",
    "    # Step 1: Formulate PubMed query\n",
    "    # query_term = ' AND '.join(keyword.replace(' ', '+') for keyword, _ in entities)\n",
    "    # time.sleep(1)  # Adjust the sleep time if needed\n",
    "    \n",
    "    # sentence=sentence.replace(' ', '+')\n",
    "    sentence = re.sub(r'\\(\"([^\"]+)\"\\)', lambda m: '(\"'+m.group(1).replace(\" \", \"+\")+'\")', sentence)\n",
    "    sentence = re.sub(\n",
    "    r'\\s*\\[(Title/Abstract|MeSH)\\]\\s*',  # 匹配 [Title/Abstract] 或 [MeSH]\n",
    "    '',                                   # 替换为空字符串\n",
    "    sentence\n",
    "    )\n",
    "    print(\"变＋号后:\",sentence)\n",
    "    query_term=sentence\n",
    "    # Step 2: Query PubMed to get PMIDs\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": query_term,\n",
    "        \"retmax\": max_length,\n",
    "        \"mindate\": MINDATE,\n",
    "        \"maxdate\": MAXDATE,\n",
    "        \"api_key\": API_KEY  # 添加 API 密钥参数\n",
    "    }\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                break  # If the request is successful, break out of the loop\n",
    "            elif response.status_code == 429:  # Too many requests\n",
    "                print(f\"Too many requests. Retrying after {attempt + 1} seconds...\")\n",
    "                time.sleep((attempt + 1) * 2)  # Exponential backoff (backoff increases with each retry)\n",
    "            else:\n",
    "                print(f\"Failed to retrieve PMIDs. Status code: {response.status_code}\")\n",
    "                return []  # If response is not 200 or 429, return empty list\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with exception: {e}. Retrying...\")\n",
    "            time.sleep((attempt + 1) * 2)  # Exponential backoff\n",
    "\n",
    "    # If we exhaust retries without success, return empty list\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve PMIDs after maximum retries.\")\n",
    "        return []\n",
    "\n",
    "    # Step 3: Parse PMIDs\n",
    "    root = ET.fromstring(response.content)\n",
    "    pmids = [id_elem.text for id_elem in root.findall('.//IdList/Id')]\n",
    "\n",
    "    # Step 4: Fetch article details\n",
    "    efetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": \",\".join(pmids),\n",
    "        \"retmode\": \"xml\",\n",
    "        \"api_key\": API_KEY  # 添加 API 密钥参数\n",
    "    }\n",
    "\n",
    "    response = requests.get(efetch_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve article details. Status code:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    # Step 5: Process XML response\n",
    "    root = ET.fromstring(response.content)\n",
    "    results = []\n",
    "\n",
    "    for article in root.findall('.//PubmedArticle'):\n",
    "        # Extract PMID\n",
    "        pmid_elem = article.find('.//PMID')\n",
    "        pmid = pmid_elem.text if pmid_elem is not None else 'N/A'\n",
    "\n",
    "        # Extract Title\n",
    "        title_elem = article.find('.//ArticleTitle')\n",
    "        title = title_elem.text if title_elem is not None else 'No title available'\n",
    "\n",
    "        # Extract Abstract\n",
    "        abstract_elem = article.find('.//Abstract')\n",
    "        abstract_parts = []\n",
    "        if abstract_elem is not None:\n",
    "            for text_elem in abstract_elem.iterfind('.//AbstractText'):\n",
    "                if text_elem.text:\n",
    "                    abstract_parts.append(text_elem.text.strip())\n",
    "        abstract = ' '.join(abstract_parts) if abstract_parts else 'No abstract available'\n",
    "        \n",
    "\n",
    "        # Build result dict (保持相同的结构)\n",
    "        result = {\n",
    "            \"id\": f\"http://www.ncbi.nlm.nih.gov/pubmed/{pmid}\",  # 保持相同的URL格式\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract  # 合并为单个字符串\n",
    "        }\n",
    "        results.append(result)\n",
    "        # print(results)\n",
    "        if verbose:\n",
    "            print(f\"Processed PMID: {pmid}\")\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Abstract: {abstract[:100]}...\\n{'-'*80}\")\n",
    "\n",
    "    print(f\"docs found: {len(results)}\")\n",
    "    return results  # 返回与第一个函数完全相同的结构\n",
    "\n",
    "\n",
    "def createQuery(query_string: str, size=50): \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": query_string\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "def expand_query_few_shot(df_prior, n, question:str, model:str):\n",
    "    messages = generate_n_shot_examples_expansion(df_prior, n)\n",
    "    # Add the user message\n",
    "    user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"\"\"\n",
    "    Given a biomedical question, generate a PubMed query string. \n",
    "    \n",
    "    **必须遵守的格式规则：**\n",
    "    **1. 最终结果必须严格包裹在##之间**\n",
    "    **2. 结果中不得包含任何解释性文字**\n",
    "    **3. 必须使用PubMed标准语法**\n",
    "    \n",
    "    PubMed语法规则：\n",
    "    - 使用括号分组术语，用AND/OR连接\n",
    "    - 用双引号包裹短语（如\"muscle weakness\"）\n",
    "    - 支持字段限定（如[Title/Abstract]）\n",
    "    \n",
    "    示例（注意格式）：\n",
    "    **问题：** 维生素D缺乏的影响？\n",
    "    **正确输出：** ##(\"vitamin d deficiency\"[MeSH] OR \"hypovitaminosis d\"[Title/Abstract]) AND (\"human health\"[Title/Abstract] OR \"physiological effects\"[MeSH])##\n",
    "    \n",
    "    请为以下问题生成查询（仅输出##包裹的最终结果）：\n",
    "    '{question}'\n",
    "    \"\"\"\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "    \n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    # 调用API生成对话\n",
    "    completion = client.chat.completions.create(\n",
    "        model='deepseek-r1:32b',\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    answer = completion.choices[0].message.content\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    return answer\n",
    "\n",
    "def expand_query_wiki(wiki_context: str, question:str, model: str)-> str:\n",
    "    # Add the user message\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        {wiki_context}\n",
    "        Answer this question: '{question}' \n",
    "        Think step by step and write an exhaustive answer explaining your reasoning\"\"\"\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "        user_message\n",
    "    ]\n",
    "    print(\"\\nMessages Expand Query:\")\n",
    "    print(messages)\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0 # randomness of completion\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    query = f\"\"\"\n",
    "        {{\n",
    "            \"query\": {{\n",
    "                \"more_like_this\" : {{\n",
    "                \"fields\" : [\"title\", \"abstract\"],\n",
    "                \"like\" : {escape_for_json(completion.choices[0].message.content)},\n",
    "                \"min_term_freq\" : 1,\n",
    "                \"min_doc_freq\": 1,\n",
    "                \"boost_terms\": 1\n",
    "                }}\n",
    "            }},\n",
    "        \"size\":100\n",
    "        }}\n",
    "    \"\"\"\n",
    "    return query\n",
    "def escape_for_json(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape the content for JSON compatibility.\n",
    "    \"\"\"\n",
    "    return text.replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\n\", \" \")\n",
    "\n",
    "def generate_n_shot_examples_expansion(df, n):\n",
    "    \n",
    "    # Initialize the system message\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    \n",
    "    # Initialize the list of messages with the system message\n",
    "    messages = [system_message]\n",
    "    \n",
    "    \n",
    "    if n< 1:\n",
    "        top_entries = pd.DataFrame()\n",
    "    else:\n",
    "        top_entries = df.sort_values(by='f1_score', ascending=False).head(n)\n",
    "    \n",
    "    # Loop through each of the top n entries and add the user and assistant messages\n",
    "    for _, row in top_entries.iterrows():\n",
    "        question = row['question_body']\n",
    "        completion = row['completion']\n",
    "        \n",
    "        # Replace problematic characters in question\n",
    "        question = question.replace(\"/\", \"\\\\\\\\/\")\n",
    "        \n",
    "        # Add the user message\n",
    "        user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        Given a biomedical question, generate an PubMed query string that incorporates synonyms and related terms to improve the search results while maintaining precision and relevance to the original question. The query should be formatted for PubMed search, which supports the following syntax:\n",
    "\n",
    "        The query string should:\n",
    "        - Use parentheses to group terms, and connect terms with 'AND', 'OR', or 'NOT'.\n",
    "        - Use '\"\"' for exact phrase matching (e.g., \"quick brown\").\n",
    "        - Use ':' to specify fields for targeted searches (e.g., title:(quick OR brown)).\n",
    "        - Include synonyms and related terms to improve recall without introducing irrelevant terms.\n",
    "        - Ensure the query is specific enough to return relevant results while not being too broad.\n",
    "\n",
    "        Example format:\n",
    "        - Question: What are the effects of vitamin D deficiency on the human body?\n",
    "        - PubMed query string: (\"vitamin d\" OR \"vitamin d3\" OR \"cholecalciferol\") AND (deficiency OR insufficiency OR \"low levels\") AND (\"effects\" OR \"impact\" OR \"consequences\") AND (\"human body\" OR \"human health\")\n",
    "\n",
    "        Example of PubMed query:\n",
    "        - (\"concizumab\" OR \"concizumab treatment\") AND (\"disease\" OR \"illness\" OR \"condition\" OR \"ailment\") AND (\"indications\" OR \"application\" OR \"efficacy\" OR \"benefit\")\n",
    "\n",
    "        Tips:\n",
    "        - Focus on the main concepts and entities in the question.\n",
    "        - Use synonyms and related terms to capture variations in terminology.\n",
    "        - Use 'AND', 'OR', 'NOT' to combine terms effectively.\n",
    "        - Ensure the query string is valid for PubMed search by using correct syntax.\n",
    "\n",
    "        Please generate a PubMed query string for the following biomedical question, and make sure to wrap the final query in ## tags:\n",
    "        '{question}'\n",
    "        \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Add the assistant message\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": completion  \n",
    "        }\n",
    "        \n",
    "        messages.extend([user_message, assistant_message])\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_query_with_no_results(question, original_query, model):\n",
    "    messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "{\"role\": \"user\", \"content\": f\"\"\"Given that the following search query has returned no documents, please generate a broader query that retains the original question's context and relevance. Return only the query that can directly be used without any explanation text. Focus on maintaining the query's precision and relevance to the original question.\n",
    "\n",
    "To generate a broader query, consider the following:\n",
    "\n",
    "Identify the main concepts in the original query and prioritize them based on their importance to the question.\n",
    "Simplify the query by removing less essential terms or concepts that might be too specific or restrictive.\n",
    "Use more general terms or synonyms for the main concepts to expand the search scope while maintaining relevance.\n",
    "Reduce the number of Boolean operators (AND, OR) to make the query less restrictive.\n",
    "If the original query includes specific drug names, genes, or proteins, consider using their classes or families instead.\n",
    "Avoid using too many search fields or specific phrases in quotes, as they can limit the search results.\n",
    "Original question: '{question}', Original query that returned no results: '{original_query}' think step by step an wrapp the improved query in ## tags:\"\"\"}\n",
    "]\n",
    "\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "\n",
    "    # 调用API生成对话\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model='deepseek-r1:32b',\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    answer = completion.choices[0].message.content\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    return answer\n",
    "def no_filtered_articles(question, relevant_articles,original_query, model):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"The current query has failed to retrieve articles containing relevant text snippets matching the question context. Generate a revised PubMed query that better targets the actual content needed to answer the question. Return only the executable query wrapped in ## without explanations.\n",
    "\n",
    "When reconstructing the query consider these critical adjustments:\n",
    "1. Analyze mismatches between query terms and actual article content\n",
    "2. Expand coverage of implicit concepts in the question\n",
    "3. Include symptom/disease/treatment terminology variants\n",
    "4. Adjust Boolean logic to bridge vocabulary gaps\n",
    "5. Add contextually related biomedical entities\n",
    "6. Maintain clinical relevance while increasing lexical coverage\n",
    "\n",
    "Evidence of failure:\n",
    "- 0 relevant snippets found in {len(relevant_articles)} filtered articles\n",
    "- Snippet matching failed for question: '{question}'\n",
    "\n",
    "Original question: '{question}'\n",
    "Original failed query: '{original_query}'\n",
    "\n",
    "Generate improved query:\"\"\"}\n",
    "]\n",
    "\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model='deepseek-r1:32b',\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    answer = completion.choices[0].message.content\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "from regex import compile as regex_compile\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 预编译正则表达式（增强版）\n",
    "CODE_BLOCK_REGEX = regex_compile(\n",
    "    r'```(?:json)?\\s*(\\{.*?\\}|\\[.*?\\])\\s*```',  # 支持数组\n",
    "    re.DOTALL\n",
    ")\n",
    "JSON_REGEX = regex_compile(\n",
    "    r'(?<!\\\\)(?:\\\\{2})*(?:{(?:[^{}]|(?R))*}|\\[(?:[^\\[\\]]|(?R))*\\])',  # 支持数组和对象\n",
    "    re.DOTALL | re.VERBOSE\n",
    ")\n",
    "\n",
    "def find_extract_json(text: str) -> str:\n",
    "    \"\"\"超强防御版 JSON 提取器\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        logger.debug(\"输入文本为空\")\n",
    "        return \"\"\n",
    "\n",
    "    # 阶段1: 提取代码块内容\n",
    "    code_block_matches = CODE_BLOCK_REGEX.findall(text)\n",
    "    for match in code_block_matches:\n",
    "        candidate = match.strip()\n",
    "        logger.debug(f\"发现代码块候选: {candidate[:100]}...\")\n",
    "        if validate_json(candidate):\n",
    "            logger.info(f\"有效代码块 JSON: {candidate[:50]}...\")\n",
    "            return candidate\n",
    "\n",
    "    # 阶段2: 深度扫描全文\n",
    "    candidates = []\n",
    "    for match in JSON_REGEX.finditer(text):\n",
    "        candidate = match.group().strip()\n",
    "        logger.debug(f\"发现全文候选: {candidate[:100]}...\")\n",
    "        if validate_json(candidate):\n",
    "            candidates.append(candidate)\n",
    "\n",
    "    # 选择策略: 优先结构完整的最短候选（长候选更可能包含无效内容）\n",
    "    if candidates:\n",
    "        logger.info(f\"发现 {len(candidates)} 个有效候选\")\n",
    "        return sorted(\n",
    "            candidates,\n",
    "            key=lambda x: (x.count('\"'), -len(x)),  # 优先双引号合法数量\n",
    "            reverse=True\n",
    "        )[0]\n",
    "    \n",
    "    # 阶段3: 终极容错模式\n",
    "    logger.warning(\"未找到有效 JSON，尝试容错解析\")\n",
    "    return force_extract_json(text)\n",
    "\n",
    "def validate_json(candidate: str) -> bool:\n",
    "    \"\"\"增强 JSON 验证\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(candidate)\n",
    "        # 验证必须包含 snippets 字段（根据业务需求）\n",
    "        if isinstance(parsed, dict) and 'snippets' in parsed:\n",
    "            # 检查 snippets 是否为列表\n",
    "            if not isinstance(parsed['snippets'], list):\n",
    "                logger.debug(f\"无效 snippets 类型: {type(parsed['snippets'])}\")\n",
    "                return False\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"验证失败: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def force_extract_json(text: str) -> str:\n",
    "    \"\"\"暴力提取模式\"\"\"\n",
    "    # 尝试提取类似 JSON 的结构\n",
    "    brackets = []\n",
    "    buffer = []\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    \n",
    "    for char in text:\n",
    "        if char in ('{', '[') and not in_string:\n",
    "            brackets.append(char)\n",
    "            buffer.append(char)\n",
    "        elif char in ('}', ']') and not in_string:\n",
    "            if not brackets:\n",
    "                continue\n",
    "            brackets.pop()\n",
    "            buffer.append(char)\n",
    "        elif char == '\"':\n",
    "            if not escape:\n",
    "                in_string = not in_string\n",
    "            buffer.append(char)\n",
    "        elif char == '\\\\':\n",
    "            escape = not escape\n",
    "        else:\n",
    "            escape = False\n",
    "            buffer.append(char)\n",
    "    \n",
    "    candidate = ''.join(buffer)\n",
    "    logger.debug(f\"暴力提取结果: {candidate[:200]}...\")\n",
    "    return candidate if candidate else \"\"\n",
    "\n",
    "from unicodedata import normalize\n",
    "def normalize_unicode_string(s, form='NFKC'):\n",
    "    normalized  = normalize('NFKD', s).encode('ascii','ignore').decode()\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def generate_n_shot_examples_extraction(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "def clean_ideal_answer(text):\n",
    "    # 使用正则表达式删除<think>标签及其内容\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    # 去除可能残留的换行符并重新整理格式\n",
    "    cleaned = re.sub(r'\\n{2,}', '\\n\\n', cleaned.strip())\n",
    "    return cleaned\n",
    "\n",
    "def extract_relevant_snippets_few_shot(examples, n, article:str, question:str, model:str) -> str:\n",
    "    \n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_extraction(examples, n)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\"Given this question: '{question}' extract relevant sentences or longer snippets from the following article that help answer the question. \n",
    "If no relevant information is present, return an empty array. Return the extracted snippets as a json string array called 'snippets'. ```{article}```\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "\n",
    "    # 调用API生成对话\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model='deepseek-r1:32b',\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    answer =completion.choices[0].message.content\n",
    "    answer=clean_ideal_answer(answer)\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        json_response = find_extract_json(answer)\n",
    "    else:\n",
    "        json_response = find_extract_json(completion.content[0].text)\n",
    "    try:\n",
    "        if isinstance(json_response, str):\n",
    "            sentences = json.loads(json_response)  # 如果是字符串，则解析为字典\n",
    "        elif isinstance(json_response, dict):\n",
    "            sentences = json_response  # 如果是字典，则直接使用\n",
    "        else:\n",
    "            print(f\"Unexpected json_response type: {type(json_response)}\")\n",
    "            sentences = {\"snippets\": []}\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response as json: {json_response}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sentences = {\"snippets\": []}\n",
    "    \n",
    "    print(\"sentences为\",sentences)\n",
    "    snippets = generate_snippets_from_sentences(article, sentences['snippets'])\n",
    "    \n",
    "    return snippets\n",
    "\n",
    "def find_offset_and_create_snippet(document_id, text, sentence, section):\n",
    "    text = normalize_unicode_string(text)\n",
    "    sentence = normalize_unicode_string(sentence)\n",
    "    offset_begin = text.find(sentence)\n",
    "    offset_end = offset_begin + len(sentence)\n",
    "    return {\n",
    "        \"document\": document_id,\n",
    "        \"offsetInBeginSection\": offset_begin,\n",
    "        \"offsetInEndSection\": offset_end,\n",
    "        \"text\": sentence,\n",
    "        \"beginSection\": section,\n",
    "        \"endSection\": section\n",
    "    }\n",
    "\n",
    "def generate_snippets_from_sentences(article, sentences):\n",
    "    snippets = []\n",
    "        # 确保字段存在且为字符串\n",
    "    article_abstract = str(article.get('abstract', ''))  # 强制转换为字符串\n",
    "    article_title = str(article.get('title', ''))       # 即使字段不存在也返回空字符串\n",
    "        # 处理规范化\n",
    "    article_abstract = normalize_unicode_string(article_abstract)\n",
    "    article_title = normalize_unicode_string(article_title)\n",
    "    # article_abstract = article.get('abstract') or ''  # This will use '' if 'abstract' is None or does not exist\n",
    "    # article_abstract = normalize_unicode_string(article_abstract)\n",
    "    # article_title = normalize_unicode_string(article.get('title'))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = normalize_unicode_string(sentence)\n",
    "        if sentence in article_title:\n",
    "            snippet = find_offset_and_create_snippet(article['id'], article['title'], sentence, \"title\")\n",
    "            snippets.append(snippet)\n",
    "        elif sentence in article_abstract:\n",
    "            snippet = find_offset_and_create_snippet(article['id'], article_abstract, sentence, \"abstract\")\n",
    "            snippets.append(snippet)\n",
    "        else:\n",
    "            print(\"\\nsentences not found in article: \"+sentence+\"\\n\")\n",
    "            print(article)\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_shot_examples_reranking(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "def clean_ideal_answer(text):\n",
    "    # 使用正则表达式删除<think>标签及其内容\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    # 去除可能残留的换行符并重新整理格式\n",
    "    cleaned = re.sub(r'\\n{2,}', '\\n\\n', cleaned.strip())\n",
    "    return cleaned\n",
    "def rerank_snippets(examples, n, snippets, question:str, model:str) -> str:\n",
    "    numbered_snippets = [{'id': idx, 'text': snippet['text']} for idx, snippet in enumerate(snippets)]\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_reranking(examples, n)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\"Given this question: '{question}' select the top 10 snippets that are most helpfull for answering this question from\n",
    "                    this list of snippets, rerank them by helpfullness: ```{numbered_snippets}``` return a json array of their ids called 'snippets'\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "    # 调用API生成对话\n",
    "    client = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"lm-studio\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model='deepseek-r1:32b',\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # client = OpenAI(api_key=\"your_key\", base_url=\"https://api.deepseek.com\")\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"deepseek-reasoner\",\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.0\n",
    "    # )\n",
    "    answer =completion.choices[0].message.content\n",
    "    answer=clean_ideal_answer(answer)\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        json_response = find_extract_json(answer)\n",
    "    else:\n",
    "        json_response = find_extract_json(completion.content[0].text)\n",
    "    \n",
    "    try:\n",
    "        snippets_reranked = json.loads(json_response)\n",
    "        # snippets_reranked = json_response\n",
    "        snippets_idx = snippets_reranked['snippets']\n",
    "        filtered_array = [snippets[i] for i in snippets_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response as json: {json_response}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        filtered_array = snippets\n",
    "        \n",
    "    return filtered_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "model_name = \"deepSeek-r1:32b\"\n",
    "model_name_extract = \"deepSeek-r1:32b\"\n",
    "model_name_rerank = \"deepSeek-r1:32b\"\n",
    "\n",
    "n_shot = 10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if '/' in model_name or ':' in model_name:\n",
    "    pickl_name = model_name.replace('/', '-').replace(':', '-')\n",
    "else:\n",
    "    pickl_name = model_name\n",
    "pickl_file = f'{pickl_name}-{n_shot}-shot.pkl'\n",
    "\n",
    "def save_state(data, file_path=pickl_file):\n",
    "    \"\"\"Save the current state to a pickle file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_state(file_path=pickl_file):\n",
    "    \"\"\"Load the state from a pickle file if it exists, otherwise return None.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except EOFError:  # Handles empty pickle file scenario\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    \"\"\"Reads a JSONL file and returns a list of examples.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_text_wrapped_in_tags(input_string):\n",
    "    # pattern = \"##(.*?)##\"\n",
    "    pattern = r\"##(.*?)##|```(.*?)```\"\n",
    "    match = re.search(pattern, input_string, re.DOTALL)  \n",
    "    if match:\n",
    "        # Remove line breaks from the matched string\n",
    "        extracted_text = match.group(1).replace('\\n', '')\n",
    "        return extracted_text\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "\n",
    "def reorder_articles_by_snippet_sequence(relevant_article_ids, snippets):\n",
    "    ordered_article_ids = []\n",
    "    mentioned_article_ids = set()\n",
    "\n",
    "    # Add article IDs in the order they appear in the snippets\n",
    "    for snippet in snippets:\n",
    "        document_id = snippet['document']\n",
    "        if document_id in relevant_article_ids and document_id not in mentioned_article_ids:\n",
    "            ordered_article_ids.append(document_id)\n",
    "            mentioned_article_ids.add(document_id)\n",
    "\n",
    "    # Add the remaining article IDs that weren't mentioned in snippets\n",
    "    for article_id in relevant_article_ids:\n",
    "        if article_id not in mentioned_article_ids:\n",
    "            ordered_article_ids.append(article_id)\n",
    "\n",
    "    return ordered_article_ids\n",
    "\n",
    "\n",
    "def get_relevant_snippets(examples, n, articles, question, model_name):\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        time.sleep(1)\n",
    "        snippets = extract_relevant_snippets_few_shot(examples, n, article, question, model_name)\n",
    "        print(\"snippets为\",snippets)\n",
    "        if snippets:\n",
    "            article['snippets'] = snippets\n",
    "            processed_articles.append(article)\n",
    "    return processed_articles\n",
    "\n",
    "# Run specific few-shot configuration\n",
    "query_examples = pd.read_csv('2024-03-26_19-24-27_claude-3-opus-20240229_11B1-10-Shot_Retrieval.csv')\n",
    "\n",
    "snip_extract_examples_file = \"Snippet_Extraction_Examples.jsonl\"     \n",
    "snip_extract_examples = read_jsonl_file(snip_extract_examples_file)\n",
    "\n",
    "snip_rerank_examples_file = \"Snippet_Reranking_Examples.jsonl\"     \n",
    "snip_rerank_examples = read_jsonl_file(snip_rerank_examples_file)\n",
    "\n",
    "def clean_ideal_answer(text):\n",
    "    # 使用正则表达式删除<think>标签及其内容\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    # 去除可能残留的换行符并重新整理格式\n",
    "    cleaned = re.sub(r'\\n{2,}', '\\n\\n', cleaned.strip())\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_question(question):\n",
    "    try:\n",
    "        query_string = \"\"\n",
    "        improved_query_string = \"\"\n",
    "        relevant_articles_ids = []\n",
    "        filtered_articles_ids = [] \n",
    "        reordered_articles_ids = []\n",
    "        relevant_snippets = []\n",
    "\n",
    "        question_id = question['id']\n",
    "        print(f\"Processing question {question_id}\")\n",
    "        wiki_context = \"\"\n",
    "\n",
    "        #0 query expansion\n",
    "        completion = expand_query_few_shot(query_examples, n_shot, question['body'], model_name)\n",
    "        clean_completion=clean_ideal_answer(completion)\n",
    "        query_string = extract_text_wrapped_in_tags(clean_completion)\n",
    "        # print('------------------------------------------------')\n",
    "        # print(query_string)\n",
    "        # print('------------------------------------------------')\n",
    "        # query = createQuery(query_string)\n",
    "        query = query_string\n",
    "        print(query)\n",
    "        print(question['body'])\n",
    "        relevant_articles = run_elasticsearch_query(query)\n",
    "        # relevant_articles = run_elasticsearch_query(question['body'])\n",
    "        if len(relevant_articles) == 0:\n",
    "            improved_query_completion = refine_query_with_no_results(question['body'], query_string, model_name)\n",
    "            improved_query_completion=clean_ideal_answer(improved_query_completion)\n",
    "            improved_query_string = extract_text_wrapped_in_tags(improved_query_completion)\n",
    "            # query = createQuery(improved_query_string)\n",
    "            query = improved_query_string\n",
    "            relevant_articles = run_elasticsearch_query(query)\n",
    "            # relevant_articles = run_elasticsearch_query(question['body'])\n",
    "            if len(relevant_articles) > 0:\n",
    "                print(\"query refinement worked\")\n",
    "        \n",
    "        relevant_articles_ids = [article['id'] for article in relevant_articles]\n",
    "        \n",
    "        #1 snippet extraction\n",
    "        filtered_articles = get_relevant_snippets(snip_extract_examples, n_shot, relevant_articles, question['body'], model_name_extract)\n",
    "        # if len(filtered_articles) == 0:\n",
    "        #     pro_improved_query_completion = no_filtered_articles(question['body'], relevant_articles,query_string, model_name)\n",
    "        #     pro_improved_query_completion=clean_ideal_answer(pro_improved_query_completion)\n",
    "        #     pro_improved_query_string = extract_text_wrapped_in_tags(pro_improved_query_completion)\n",
    "        #     # query = createQuery(improved_query_string)\n",
    "        #     query = pro_improved_query_string\n",
    "        #     relevant_articles = run_elasticsearch_query(query)\n",
    "        #     relevant_articles_ids = [article['id'] for article in relevant_articles]\n",
    "        #     filtered_articles = get_relevant_snippets(snip_extract_examples, n_shot, relevant_articles, question['body'], model_name_extract)\n",
    "        max_iterations = 2  # 设置最大循环次数\n",
    "        iterations = 0  # 初始化循环次数计数器\n",
    "        \n",
    "        while len(filtered_articles) <1 and iterations < max_iterations:\n",
    "            improved_query_completion =refine_query_with_no_results(question['body'], query, model_name)\n",
    "            improved_query_completion=clean_ideal_answer(improved_query_completion)\n",
    "            improved_query_string = extract_text_wrapped_in_tags(improved_query_completion)\n",
    "            # query = createQuery(improved_query_string)\n",
    "            query = improved_query_string\n",
    "            relevant_articles = run_elasticsearch_query(query)\n",
    "            # relevant_articles = run_elasticsearch_query(question['body'])\n",
    "            relevant_articles_ids = [article['id'] for article in relevant_articles]\n",
    "            filtered_articles = get_relevant_snippets(snip_extract_examples, n_shot, relevant_articles, question['body'], model_name_extract)\n",
    "            iterations += 1  # 增加循环次数\n",
    "            if len(filtered_articles) >=1:\n",
    "                print(\"filtered_articles have one!\")\n",
    "        filtered_articles_ids = [article['id'] for article in filtered_articles]\n",
    "        relevant_snippets = [snippet for article in filtered_articles for snippet in article['snippets']]\n",
    "        print(\"relevant_snippets:\",relevant_snippets)\n",
    "        \n",
    "    \n",
    "        #2 rerank snippets\n",
    "        reranked_snippets = rerank_snippets(snip_rerank_examples, n_shot, relevant_snippets, question['body'], model_name_rerank)\n",
    "        \n",
    "        reordered_articles_ids = reorder_articles_by_snippet_sequence(filtered_articles_ids, reranked_snippets)\n",
    "\n",
    "        return {\n",
    "            \"question_id\": question[\"id\"],\n",
    "            \"question_body\": question[\"body\"],\n",
    "            \"question_type\": question[\"type\"],\n",
    "            \"wiki_context\": wiki_context,\n",
    "            \"completion\": completion,\n",
    "            \"query\": query_string,\n",
    "            \"improved_query\": improved_query_string,\n",
    "            \"relevant_articles\": relevant_articles_ids,\n",
    "            \"filtered_articles\": filtered_articles_ids,\n",
    "            \"documents\": reordered_articles_ids,\n",
    "            \"snippets\": reranked_snippets\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {question['id']}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"question_id\": question.get(\"id\", \"error\"),\n",
    "            \"question_body\": question.get(\"body\", \"error\"),\n",
    "            \"question_type\": question.get(\"type\", \"error\"),\n",
    "            \"query\": query_string or \"error\",\n",
    "            \"improved_query\": improved_query_string or \"error\",\n",
    "            \"relevant_articles\": relevant_articles_ids or [],\n",
    "            \"filtered_articles\": filtered_articles_ids or [],\n",
    "            \"documents\": reordered_articles_ids[:10] if reordered_articles_ids else [],\n",
    "            \"snippets\": relevant_snippets or []\n",
    "        }\n",
    "\n",
    "# Define columns\n",
    "columns = ['question_id', 'question_body', 'question_type', 'wiki_context', 'completion', 'query', 'improved_query', 'relevant_articles', 'filtered_articles', 'documents', 'snippets']\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "questions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Load the input file in JSON format\n",
    "input_file_name = '/workspace/code/bioasq2024/02_12B/Batch1/PhaseA/BioASQ-task13bPhaseA-testset1.json'\n",
    "\n",
    "\n",
    "with open(input_file_name) as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "# Assuming 'load_state' returns a DataFrame or None\n",
    "saved_df = load_state(pickl_file)\n",
    "\n",
    "if saved_df is not None and not saved_df.empty:\n",
    "    processed_ids = set(saved_df['question_id'])  # Assuming 'question_id' is your identifier\n",
    "    questions_df = saved_df\n",
    "else:\n",
    "    processed_ids = set()\n",
    "\n",
    "# Assuming `data[\"questions\"]` is your list of questions to process\n",
    "# Filter out questions that have already been processed\n",
    "questions_to_process = [q for q in data[\"questions\"] if q[\"id\"] not in processed_ids]\n",
    "#questions_to_process = questions_to_process[:2]\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to process questions in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Dictionary to keep track of question futures\n",
    "    future_to_question = {executor.submit(process_question, q): q for q in questions_to_process}\n",
    "    \n",
    "    for future in as_completed(future_to_question):\n",
    "        question = future_to_question[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Append result to the DataFrame\n",
    "                result_df = pd.DataFrame([result])\n",
    "                questions_df = pd.concat([questions_df, result_df], ignore_index=True)\n",
    "                save_state(questions_df, pickl_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {question['id']}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "if '/' in model_name:\n",
    "    model_name_pretty = model_name.split(\"/\")[-1]\n",
    "else:\n",
    "    model_name_pretty = model_name\n",
    "output_file_name = f\"./Results/{timestamp}_{model_name_pretty}_2024AB1-Fine-Tuned-{n_shot}-Shot.csv\"\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "os.makedirs(os.path.dirname(output_file_name), exist_ok=True)\n",
    "\n",
    "questions_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# After processing all questions and saving the final output:\n",
    "try:\n",
    "    # Check if the pickle file exists before attempting to delete it\n",
    "    if os.path.exists(pickl_file):\n",
    "        os.remove(pickl_file)\n",
    "        print(\"Intermediate state pickle file deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting pickle file: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Run File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_json(csv_filepath, json_filepath):\n",
    "    empty = 0\n",
    "    # Step 1: Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    \n",
    "    # Transform the DataFrame into a list of dictionaries, one per question\n",
    "    questions_list = df.to_dict(orient='records')\n",
    "    \n",
    "    # Initialize the structure of the JSON file\n",
    "    json_structure = {\"questions\": []}\n",
    "    \n",
    "    # Step 2: Transform the DataFrame into the desired JSON structure\n",
    "    for item in questions_list:\n",
    "        # Adjusting exact_answer format based on question_type\n",
    "        if item[\"question_type\"] in [\"list\", \"factoid\"]:\n",
    "            exact_answer_format = [[]]  # For 'list' or 'factoid', it's a list of lists\n",
    "        else:\n",
    "            exact_answer_format = \"\"  # Default to an empty string\n",
    "            \n",
    "            \n",
    "        if len(eval(item[\"relevant_articles\"])) == 0:\n",
    "            empty = empty +1\n",
    "        #print(len(eval(item[\"relevant_articles\"])))\n",
    "        # Construct question_dict conditionally excluding 'exact_answer' for 'ideal' type\n",
    "        question_dict = {\n",
    "            \"documents\": eval(item[\"documents\"])[:10],\n",
    "            \"snippets\": eval(item[\"snippets\"])[:10],\n",
    "            \"body\": item[\"question_body\"],\n",
    "            \"type\": item[\"question_type\"],\n",
    "            \"id\": item[\"question_id\"],\n",
    "            \"ideal_answer\": \"\"\n",
    "        }\n",
    "        if item[\"question_type\"] != \"summary\":\n",
    "            question_dict[\"exact_answer\"] = exact_answer_format\n",
    "        \n",
    "        json_structure[\"questions\"].append(question_dict)\n",
    "    \n",
    "    # Step 3: Write the JSON structure to a file\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_structure, json_file, ensure_ascii=False, indent=4)\n",
    "    print(empty)\n",
    "\n",
    "# Example usage\n",
    "csv_filepath = '/workspace/code/bioasq2024/02_12B/Batch1/PhaseA/Results/2025-05-07_16-30-34_deepSeek-r1:32b_2024AB1-Fine-Tuned-1-Shot.csv'  # Update this path to your actual CSV file path\n",
    "json_filepath = '/workspace/code/bioasq2024/02_12B/Batch1/PhaseA/Results/bioasq13b-batch4-deepseek-32b.json'  # Update this path to where you want to save the JSON file\n",
    "csv_to_json(csv_filepath, json_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = '{\"snippets\": [\"In this narrative review, we summarize the current management of HER2-low breast cancer. We highlight the findings of the DESTINY-Breast 04\\\\xa0phase 3 trial, which confirmed the efficacy of trastuzumab-deruxtecan (T-DXd) for the treatment of patients with advanced, pretreated HER2-low breast cancer.\"]}'\n",
    "test_string2 = '{\"snippets\": [\"The limit of detection (LOD) and the limit of quantification (LOQ) for the target DNA could reach 32 pM and 1 nM, respectively.\"],\"additional_properties\":{\"pubmed_id\":\"33303163\"}'\n",
    "test_string3 = '{\"snippets\": [\"The limit of blank (LOB) was 3\\tµg calprotectin/g faeces (µg/g), LOD 8\\tμg/g and LOQ 20\\tμg/g.\"]}'\n",
    "test_string4 = '{\"snippets\": [\"In this narrative review, we summarize the current management of HER2-low breast cancer. We highlight the findings of the DESTINY-Breast 04\\\\xa0phase 3 trial, which confirmed the efficacy of trastuzumab-deruxtecan (T-DXd) for the treatment of patients with advanced, pretreated HER2-low breast cancer.\"]}'\n",
    "test_string4 = test_string4.replace('\\\\', \"\\\\\\\\\")\n",
    "print(test_string4)\n",
    "json.loads(test_string4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scispacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
